{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlayWithCifar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymittal23/PlayWithCifar/blob/master/PlayWithCifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUQJgoWh0gvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time, math\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow.contrib.eager as tfe\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4d26PCi0k8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.4 \n",
        "epochs = 25\n",
        "batch_size = 512\n",
        "end_percentage = 0.05\n",
        "triangle_tilt = 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojbPZ4u0q9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the CIFAR10 60000 Training and 10000 Test data into respective numpy arrays\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
        "img_size = X_train.shape[1]\n",
        "n_classes = y_train.max() + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMUvQupf0sI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "Y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRkj-8AC0198",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68550a0f-d641-4ef1-f8da-61e3c9eaaa61"
      },
      "source": [
        "X_train_mean = np.mean(X_train, axis=(0,1,2))\n",
        "X_train_std = np.std(X_train, axis=(0,1,2))\n",
        "print(X_train_mean, X_train_std)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.4914009  0.48215896 0.4465308 ] [0.24703279 0.24348423 0.26158753]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORzn74vX0592",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf285bc-2fba-478d-ea25-b2793c556ec1"
      },
      "source": [
        "X_test_mean = np.mean(X_test, axis=(0,1,2))\n",
        "X_test_std = np.std(X_test, axis=(0,1,2))\n",
        "print(X_test_mean, X_test_std)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.49421427 0.4851322  0.45040992] [0.24665268 0.24289216 0.2615922 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB6zp_XJ08u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = (X_train - X_train_mean) / X_train_std\n",
        "X_test = (X_test - X_test_mean) / X_test_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkXArqHG1XBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(input_image,padding_pixels=4,random_crop_size=(32,32)):\n",
        "          # Note: image_data_format is 'channel_last'\n",
        "          assert input_image.shape[2] == 3\n",
        "          \n",
        "          #Pad by 4 pixels\n",
        "          img = cv2.copyMakeBorder(input_image, padding_pixels, padding_pixels, padding_pixels, padding_pixels, cv2.BORDER_REPLICATE)\n",
        "          \n",
        "          height, width = img.shape[0], img.shape[1]\n",
        "          dy, dx = random_crop_size\n",
        "          x = np.random.randint(0, width - dx + 1)\n",
        "          y = np.random.randint(0, height - dy + 1)\n",
        "          return img[y:(y+dy), x:(x+dx), :]\n",
        "        \n",
        "tr_seq = list(range(len(X_train)))\n",
        "train_func = lambda i:random_crop(X_train[i])\n",
        "train_features = list(map(train_func,tr_seq))\n",
        "\n",
        "train_features = np.asarray(train_features)\n",
        "train_labels = Y_train "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOsPEpu41iYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_padcrop(p=0.5, s_l=0.3, s_h=0.3, r_1=0.3, r_2=1 / 0.3, max_erasures_per_image=1, pixel_level=True, random_crop_size=(32, 32), padding_pixels=4):\n",
        "    \"\"\"\n",
        "    :param p:\n",
        "    :param s_l: Minimum Area Proportion of Original that may be cut\n",
        "    :param s_h: Maximum Area Proportion of Original that may be cut\n",
        "    :param r_1: Min Aspect Ratio\n",
        "    :param r_2: Max Aspect Ratio\n",
        "    :param max_erasures_per_image:\n",
        "    :param pixel_level:\n",
        "    :return: Eraser to be used as Preprocessing Function\n",
        "    \"\"\"\n",
        "    assert max_erasures_per_image >= 1\n",
        "\n",
        "    def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        mx = np.random.randint(1, max_erasures_per_image + 1)\n",
        "        # print(\"Erasures = \",mx,end =\", \")\n",
        "        for i in range(mx):\n",
        "            while True:\n",
        "                s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "                r = np.random.uniform(r_1, r_2)\n",
        "                w = int(np.sqrt(s / r))\n",
        "                h = int(np.sqrt(s * r))\n",
        "                left = np.random.randint(0, img_w)\n",
        "                top = np.random.randint(0, img_h)\n",
        "\n",
        "                if left + w <= img_w and top + h <= img_h:\n",
        "                    break\n",
        "\n",
        "            # print(\"W = \",w,\"H = \",h,end =\", \")\n",
        "\n",
        "            if pixel_level:\n",
        "                # print(np.max(img_c),np.min(img_c),v_l,v_h)\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "                # print(c.shape,np.min(c),np.max(c),np.median(c))\n",
        "            else:\n",
        "                c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "            input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        # print()\n",
        "        return input_img\n",
        "     \n",
        "    \n",
        "        \n",
        "    def preproc_image(input_image):\n",
        "      #return eraser\n",
        "      return eraser(input_image)\n",
        "\n",
        "    return preproc_image\n",
        "      \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYfZexfT1lib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "   #featurewise_center=True,\n",
        "   #featurewise_std_normalization=True,\n",
        "   horizontal_flip=0.5,                 # randomly flip images                                     \n",
        "   preprocessing_function=get_cutout_eraser_and_padcrop(p=0.8, s_l=0.25, s_h=0.25, r_1=0.2, r_2=1 / 0.3, max_erasures_per_image=1, pixel_level=False))\n",
        "\n",
        "_ = datagen.fit(X_train)\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=256, shuffle=False)\n",
        "\n",
        "# X_e, Y_e = train_iterator.next()\n",
        "# X_e = min_max_scale(X_e)\n",
        "# show_examples(X_e[0:10], Y_e[0:10], classes = get_cifar10_labels())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Up4S2AB1oyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen_validation = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "datagen_validation.fit(X_test)\n",
        "validation_iterator = datagen_validation.flow(X_test, Y_test, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tLfBhaH1fZQ",
        "colab_type": "text"
      },
      "source": [
        "Custom Image Data Generator for Augmentation\n",
        "\n",
        "Here storing all the augmented images and later will pick up half augmented from here and half from original images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR_5k8iU1rui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "ecc14ee1-ad86-4341-c0e1-cc50578e0ece"
      },
      "source": [
        "import h5py\n",
        "f = h5py.File('aug_img.hdf5', 'w')\n",
        "d = f.create_dataset('dataset', (600000,32,32,3),chunks=(256,32,32,3))\n",
        "l = f.create_dataset('labels',(600000,10),chunks=(256,10))\n",
        "\n",
        "\n",
        "batch_size=256\n",
        "EPOCHS =12\n",
        "aug_lbl=[]\n",
        "tmp=0\n",
        "for i in range(EPOCHS):\n",
        "  print(\"tmp \", tmp)\n",
        "  for j in range(len(train_iterator)):\n",
        "    initial = j*batch_size+tmp\n",
        "    train_gen = train_iterator.next()\n",
        "    if(j == 195 ):\n",
        "      final += 80\n",
        "      d[initial:final]= train_gen[0]\n",
        "      l[initial:final]= train_gen[1]\n",
        "      continue \n",
        "      \n",
        "    \n",
        "    final = initial + 256\n",
        "    d[initial:final]= train_gen[0]\n",
        "    l[initial:final]= train_gen[1]\n",
        "  \n",
        "  tmp = final\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tmp  0\n",
            "tmp  50000\n",
            "tmp  100000\n",
            "tmp  150000\n",
            "tmp  200000\n",
            "tmp  250000\n",
            "tmp  300000\n",
            "tmp  350000\n",
            "tmp  400000\n",
            "tmp  450000\n",
            "tmp  500000\n",
            "tmp  550000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw-1-bEK1wFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(32, 32, 3)\n",
        "num_outputs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIxi4KZa1tFC",
        "colab_type": "text"
      },
      "source": [
        "## David Net Architecture "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTLYDkjH13j4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c49c1a60-7da6-4423-83b6-cb54a482816c"
      },
      "source": [
        "def block(input_layer,filters,stride=1):\n",
        "  \n",
        "  conv_1 = tf.keras.layers.Conv2D(filters, kernel_size=(3,3), padding='same', strides=stride, kernel_regularizer=l2(0.0001))(input_layer)\n",
        "  \n",
        "  bn_1 = tf.keras.layers.BatchNormalization(axis=3,momentum=0.9,epsilon=1e-5)(conv_1)\n",
        "  \n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  \n",
        "  return activation_layer_b1\n",
        "\n",
        "\n",
        "input = tf.keras.layers.Input(shape=input_shape)\n",
        "start = block(input,32)\n",
        "\n",
        "layer_1 = block(start,64)\n",
        "mp_1 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(layer_1)\n",
        "\n",
        "layer1_identity = tf.keras.layers.Conv2D(filters=32,kernel_size=(1, 1),strides=(1, 1),padding=\"same\",kernel_regularizer=l2(0.0001))(mp_1)\n",
        "layer1_res1 = block(layer1_identity,64)\n",
        "layer1_res2 = block(layer1_res1,64)\n",
        "\n",
        "concat1 = tf.keras.layers.concatenate([mp_1, layer1_res2])\n",
        "\n",
        "layer_2 = block(concat1,128)\n",
        "mp_2 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(layer_2)\n",
        "\n",
        "layer_3 = block(mp_2,256)\n",
        "mp_3 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(layer_3)\n",
        "\n",
        "layer3_identity = tf.keras.layers.Conv2D(filters=128,kernel_size=(1, 1),strides=(1, 1),padding=\"same\",kernel_regularizer=l2(0.0001))(mp_3)\n",
        "layer3_res1 = block(layer3_identity,256)\n",
        "layer3_res2 = block(layer3_res1,256)\n",
        "\n",
        "concat2 = tf.keras.layers.concatenate([mp_3, layer3_res2])\n",
        "\n",
        "gmp = tf.keras.layers.GlobalAveragePooling2D()(concat2)\n",
        "dense = tf.keras.layers.Dense(units=num_outputs, activation=\"softmax\")(gmp) #kernel_initializer=\"he_normal\", \n",
        "\n",
        "model = tf.keras.models.Model(inputs=input, outputs=dense)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNNMH5M14wo",
        "colab_type": "text"
      },
      "source": [
        "For slanted learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38lNPmOD16gb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "6000d5a9-c400-420b-f4d8-8007a3ef81f5"
      },
      "source": [
        "def min_max_scaler(iterations, num_iterations, end_percentage, min_val, max_val, invert = False, triangle_tilt = 0.65):\n",
        "  non_slant_mid_cycle_id = int(num_iterations * ((1. - end_percentage)) / float(2))\n",
        "  mid_cycle_id = int(triangle_tilt*int(num_iterations * ((1. - end_percentage)) / float(2)))\n",
        "  value = 0\n",
        "  if iterations > 2 * non_slant_mid_cycle_id:\n",
        "    \n",
        "    \n",
        "      extra_iters = (iterations - 2 * non_slant_mid_cycle_id)\n",
        "      current_percentage = 1 - (1 - 1/10)*extra_iters/(num_iterations - 2 * non_slant_mid_cycle_id)\n",
        "            \n",
        "\n",
        "  elif iterations >  mid_cycle_id:\n",
        "      current_percentage = 1. - (iterations - mid_cycle_id) / (2*non_slant_mid_cycle_id - mid_cycle_id)\n",
        "      \n",
        "  else:\n",
        "      current_percentage = iterations / mid_cycle_id\n",
        "      \n",
        "  if invert:\n",
        "    if iterations > 2 * non_slant_mid_cycle_id:\n",
        "      return max_val\n",
        "    return max_val - current_percentage * (max_val - min_val)\n",
        "  else:\n",
        "    if iterations > 2 * non_slant_mid_cycle_id:\n",
        "      return min_val * current_percentage\n",
        "    return min_val + current_percentage * (max_val - min_val)\n",
        "  \n",
        "    \n",
        "\n",
        "print(\"=\"*80)\n",
        "scales = []\n",
        "for i in range(1000):\n",
        "  p = min_max_scaler(i,1000,0.1,min_val=0.1,max_val=1, invert=False)\n",
        "  scales.append(p)\n",
        "  \n",
        "plt.plot(np.array(scales))\n",
        "plt.title(\"LR Graph\")\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfxySZYm2B7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "\n",
        "\n",
        "# Code is ported from https://github.com/fastai/fastai\n",
        "class OneCycleLR(Callback):\n",
        "    def __init__(self,\n",
        "                 epochs,\n",
        "                 batch_size,\n",
        "                 samples,\n",
        "                 steps,\n",
        "                 max_lr,\n",
        "                 end_percentage=0.1,\n",
        "                 scale=100,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 triangle_tilt = 0.75,\n",
        "                 verbose=True):\n",
        "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
        "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
        "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
        "        100th its initial lowest value.\n",
        "        # Arguments:\n",
        "            max_lr: Float. Initial learning rate. This also sets the\n",
        "                starting learning rate (which will be 10x smaller than\n",
        "                this), and will increase to this value during the first cycle.\n",
        "            end_percentage: Float. The percentage of all the epochs of training\n",
        "                that will be dedicated to sharply decreasing the learning\n",
        "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
        "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
        "                If None, it will compute the scale_percentage automatically\n",
        "                based on the `end_percentage`.\n",
        "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
        "                value, which gradually drops to its lowest value in half-cycle,\n",
        "                then gradually increases again to stay constant at this max value.\n",
        "                Can only be used with SGD Optimizer.\n",
        "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
        "                the half-cycle. Can only be used with SGD Optimizer.\n",
        "            verbose: Bool. Whether to print the current learning rate after every\n",
        "                epoch.\n",
        "        # Reference\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
        "        \"\"\"\n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = scale\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = samples\n",
        "        self.steps = steps\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the learning rate based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the learning rate gradually increases.\n",
        "        - If in the second half of training, the learning rate gradually decreases.\n",
        "        - If in the final `end_percentage` portion of training, the learning rate\n",
        "            is quickly reduced to near 100th of the original min learning rate.\n",
        "        # Returns:\n",
        "            the new learning rate\n",
        "        \"\"\"\n",
        "        new_lr = min_max_scaler(self.clr_iterations, self.num_iterations, self.end_percentage, self.initial_lr/self.scale, self.initial_lr, invert = False, triangle_tilt=triangle_tilt)\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "        \"\"\"\n",
        "         Compute the momentum based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the momentum gradually decreases.\n",
        "        - If in the second half of training, the momentum gradually increases.\n",
        "        - If in the final `end_percentage` portion of training, the momentum value\n",
        "            is kept constant at the maximum initial value.\n",
        "        # Returns:\n",
        "            the new momentum value\n",
        "        \"\"\"    \n",
        "        new_momentum = min_max_scaler(self.clr_iterations, self.num_iterations, self.end_percentage, self.min_momentum, self.max_momentum, invert = True, triangle_tilt=triangle_tilt)\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "                \n",
        "    \n",
        "    def plot(self):\n",
        "        plt.title(\"LR-Plot\")\n",
        "        plt.plot(self.history['lr'])\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"LR\")\n",
        "        plt.show()\n",
        "        \n",
        "        plt.title(\"Momentum-Plot\")\n",
        "        plt.plot(self.history['momentum'])\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Momentum\")\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD6gGKdC2Bdm",
        "colab_type": "text"
      },
      "source": [
        "One Cycle LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUvErBGx2D34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "olr = OneCycleLR(epochs=24, batch_size = batch_size,steps=98, \n",
        "                 samples=X_train.shape[0], max_lr=0.6, verbose = True, scale = 50, end_percentage=0.1,\n",
        "                 maximum_momentum = 0.95, minimum_momentum=0.85, triangle_tilt=0.75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOtGYrC92Hvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = tf.keras.optimizers.SGD(lr=0.08, decay=5e-4, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p659c6q12osV",
        "colab_type": "text"
      },
      "source": [
        "Reading the HDF5 file of Augmented Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkbCDC7p2Jt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = tf.keras.utils.HDF5Matrix('./aug_img.hdf5','dataset')\n",
        "label = tf.keras.utils.HDF5Matrix('./aug_img.hdf5','labels')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt3QSwgy20tD",
        "colab_type": "text"
      },
      "source": [
        "#Custom Data Generator\n",
        "\n",
        "It picks half of batch from augmented images and other half from original images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyeIAgi92_yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#import keras\n",
        "import random\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, data, label, train_features,train_labels, batch_size=512, dim=(32,32), n_channels=3,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.label = label\n",
        "        self.data = data\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.train_features = train_features\n",
        "        self.train_labels = train_labels\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        \n",
        "        # Generate indexes of the batch\n",
        "        #indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        aug_ran = random.randrange(0,len(self.data)-256)\n",
        "        wo_aug = random.randrange(0,len(self.train_features)-256)\n",
        "        #print(aug_ran)\n",
        "        train_data_aug = lambda i: (np.array(self.data[i:i+256]),np.array(self.label[i:i+256]))\n",
        "        train_data_wo = lambda i: (np.array(self.train_features[i:i+256]),np.array(self.train_labels[i:i+256]))\n",
        "        \n",
        "        \n",
        "        X = np.concatenate((train_data_aug(aug_ran)[0],train_data_wo(wo_aug)[0]),axis=0)\n",
        "        y = np.concatenate((train_data_aug(aug_ran)[1],train_data_wo(wo_aug)[1]),axis=0)\n",
        "        #print(\"label\",y.shape, \" \" ,X.shape)\n",
        "        #plt.imshow(X[0])\n",
        "        # Generate data\n",
        "        #X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "            \n",
        "gen = DataGenerator(data,label,train_features,train_labels,512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAiG0vtu2PU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d85f243-06ed-46ac-b564-3b43f9d4400d"
      },
      "source": [
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "\n",
        "model_info = model.fit_generator(gen,\n",
        "                                 steps_per_epoch = 98, epochs = 24, \n",
        "                                 validation_data = validation_iterator, \n",
        "                                 validation_steps = len(validation_iterator),\n",
        "                                 verbose=1, callbacks=[olr])\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "# plot_model_history(model_info)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 1.6437 - acc: 0.4436 - lr: 0.08908 - momentum: 0.94 \n",
            "98/98 [==============================] - 31s 315ms/step - loss: 1.6390 - acc: 0.4457 - val_loss: 1.3185 - val_acc: 0.5769\n",
            "Epoch 2/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 1.1461 - acc: 0.6296 - lr: 0.16695 - momentum: 0.92 \n",
            "98/98 [==============================] - 23s 233ms/step - loss: 1.1445 - acc: 0.6302 - val_loss: 1.5757 - val_acc: 0.5862\n",
            "Epoch 3/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.9367 - acc: 0.7155 - lr: 0.24482 - momentum: 0.91 \n",
            "98/98 [==============================] - 23s 232ms/step - loss: 0.9364 - acc: 0.7157 - val_loss: 1.3299 - val_acc: 0.6459\n",
            "Epoch 4/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.8390 - acc: 0.7542 - lr: 0.32269 - momentum: 0.90 \n",
            "98/98 [==============================] - 23s 234ms/step - loss: 0.8377 - acc: 0.7545 - val_loss: 1.0093 - val_acc: 0.7002\n",
            "Epoch 5/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7811 - lr: 0.40056 - momentum: 0.88 \n",
            "98/98 [==============================] - 23s 236ms/step - loss: 0.7709 - acc: 0.7811 - val_loss: 0.9774 - val_acc: 0.7357\n",
            "Epoch 6/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.7221 - acc: 0.8010 - lr: 0.47843 - momentum: 0.87 \n",
            "98/98 [==============================] - 23s 236ms/step - loss: 0.7216 - acc: 0.8013 - val_loss: 0.8156 - val_acc: 0.7858\n",
            "Epoch 7/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.6873 - acc: 0.8171 - lr: 0.55630 - momentum: 0.86 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.6871 - acc: 0.8171 - val_loss: 0.9670 - val_acc: 0.7624\n",
            "Epoch 8/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.6509 - acc: 0.8332 - lr: 0.58162 - momentum: 0.85 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.6503 - acc: 0.8334 - val_loss: 0.7735 - val_acc: 0.8041\n",
            "Epoch 9/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.6119 - acc: 0.8479 - lr: 0.53975 - momentum: 0.86 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.6120 - acc: 0.8480 - val_loss: 0.6978 - val_acc: 0.8302\n",
            "Epoch 10/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.8586 - lr: 0.49787 - momentum: 0.87 \n",
            "98/98 [==============================] - 23s 234ms/step - loss: 0.5732 - acc: 0.8587 - val_loss: 0.7363 - val_acc: 0.8269\n",
            "Epoch 11/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.8743 - lr: 0.45599 - momentum: 0.87 \n",
            "98/98 [==============================] - 23s 234ms/step - loss: 0.5441 - acc: 0.8739 - val_loss: 0.6493 - val_acc: 0.8526\n",
            "Epoch 12/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8777 - lr: 0.41411 - momentum: 0.88 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.5298 - acc: 0.8779 - val_loss: 0.6907 - val_acc: 0.8385\n",
            "Epoch 13/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8878 - lr: 0.37224 - momentum: 0.89 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.4999 - acc: 0.8882 - val_loss: 0.6050 - val_acc: 0.8647\n",
            "Epoch 14/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.4759 - acc: 0.8978 - lr: 0.33036 - momentum: 0.90 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.4756 - acc: 0.8980 - val_loss: 0.6212 - val_acc: 0.8582\n",
            "Epoch 15/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.9025 - lr: 0.28848 - momentum: 0.90 \n",
            "98/98 [==============================] - 23s 234ms/step - loss: 0.4570 - acc: 0.9029 - val_loss: 0.6071 - val_acc: 0.8629\n",
            "Epoch 16/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.9114 - lr: 0.24660 - momentum: 0.91 \n",
            "98/98 [==============================] - 23s 236ms/step - loss: 0.4310 - acc: 0.9112 - val_loss: 0.5888 - val_acc: 0.8724\n",
            "Epoch 17/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.9219 - lr: 0.20472 - momentum: 0.92 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.4066 - acc: 0.9220 - val_loss: 0.5435 - val_acc: 0.8796\n",
            "Epoch 18/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.9266 - lr: 0.16285 - momentum: 0.92 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.3898 - acc: 0.9266 - val_loss: 0.5670 - val_acc: 0.8746\n",
            "Epoch 19/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.9362 - lr: 0.12097 - momentum: 0.93 \n",
            "98/98 [==============================] - 23s 236ms/step - loss: 0.3582 - acc: 0.9364 - val_loss: 0.5264 - val_acc: 0.8866\n",
            "Epoch 20/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.9392 - lr: 0.07909 - momentum: 0.94 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.3471 - acc: 0.9393 - val_loss: 0.5053 - val_acc: 0.8963\n",
            "Epoch 21/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.9505 - lr: 0.03721 - momentum: 0.95 \n",
            "98/98 [==============================] - 23s 233ms/step - loss: 0.3155 - acc: 0.9506 - val_loss: 0.4897 - val_acc: 0.8999\n",
            "Epoch 22/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9567 - lr: 0.01022 - momentum: 0.95 \n",
            "98/98 [==============================] - 23s 235ms/step - loss: 0.2951 - acc: 0.9567 - val_loss: 0.4663 - val_acc: 0.9085\n",
            "Epoch 23/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9611 - lr: 0.00573 - momentum: 0.95 \n",
            "98/98 [==============================] - 23s 234ms/step - loss: 0.2842 - acc: 0.9613 - val_loss: 0.4590 - val_acc: 0.9098\n",
            "Epoch 24/24\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9639 - lr: 0.00125 - momentum: 0.95 \n",
            "98/98 [==============================] - 23s 234ms/step - loss: 0.2751 - acc: 0.9639 - val_loss: 0.4596 - val_acc: 0.9110\n",
            "Model took 563.41 seconds to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-gqSZrn2SgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a022ba2-9010-4ae0-e93f-4f69e4ca5db7"
      },
      "source": [
        "# compute test accuracy\n",
        "result = model.evaluate_generator(validation_iterator, steps = len(validation_iterator))\n",
        "print(result)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.45956436097621917, 0.911]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrhiR-0y7h25",
        "colab_type": "text"
      },
      "source": [
        "So this gave me accuracy around 91-92%. Model is trained for just 563 seconds with 24 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5MhrhIh4h4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}